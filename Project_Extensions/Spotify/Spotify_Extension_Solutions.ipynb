{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to install the necessary dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datascience import *\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to load the special plotting functions\n",
    "def quant_data_bar(tbl, column_label, num_bins=10, x_min=None, x_max=None):\n",
    "    \"\"\"\n",
    "    Plots a bar chart of quantitative data for a given input table and desired column.\n",
    "    This function will bin the values of column_label and plot the counts of each bin on the\n",
    "    y-axis.\n",
    "    \n",
    "    Parameters:\n",
    "        tbl: A Table containing the data you would like to plot\n",
    "        column_label: The name of the column to be binned and plotted on the x-axis\n",
    "        num_bins (optional): The number of equal-width bins. By default, this function\n",
    "                                uses 10 bins.\n",
    "        x_min (optional): Desired lowest value on the horizontal axis; default is the \n",
    "                            minimum of the data in the specified column\n",
    "        x_max (optional): Desired highest value on the horizontal axis; default is the \n",
    "                            maximum of the data in the specified column\n",
    "    \"\"\"\n",
    "    tbl.hist(column_label, normed=False, bins=num_bins)\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim(left=x_min) if x_min else None\n",
    "    ax.set_xlim(right=x_max) if x_max else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Above and Beyond*: Spotify Project Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Science Life Cycle - Table of Contents\n",
    "\n",
    "<a href='#section 0'>Background Knowledge</a>\n",
    "\n",
    "<a href='#subsection 1a'>Formulating a question or problem</a> \n",
    "\n",
    "<a href='#subsection 1b'>Acquiring and preparing data</a>\n",
    "\n",
    "<a href='#subsection 1c'>Conducting exploratory data analysis</a>\n",
    "\n",
    "<a href='#subsection 1d'>Using prediction and inference to draw conclusions</a>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background Knowledge <a id='section 0'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you listen to music, chances are you use Spotify, Apple Music, or another similar streaming service. This new era of the music industry curates playlists, recommends new artists, and is based on the number of streams more than the number of albums sold. The way these streaming services do this is (you guessed it) data!\n",
    "\n",
    "Spotify, like many other companies, hire many full-time data scientists to analyze all the incoming user data and use it to make predictions and recommendations for users. If you're interested, feel free to check out [Spotify's Engineering Page](https://engineering.atspotify.com/) for more information!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/spotify.png\" width = 700/>\n",
    "\n",
    "<center><a href=https://hrblog.spotify.com/2018/02/08/amping-up-diversity-inclusion-at-spotify/>Image Reference</a></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background Readings\n",
    "\n",
    "The following articles, which talk about identifying musical genres using machine learning and artificial intelligence, will help provide context to this notebook:\n",
    "1. [AI Detecting Genres](https://www.engadget.com/2017-05-04-machine-learning-ai-identifies-music-genres.html): An AI can recognize musical genres better than humans, but it won’t shame you for liking that one Justin Bieber song.\n",
    "\n",
    "2. [Dubolt](https://dubolt.com/): A web application that uses Spotify audio features to recommend songs!\n",
    "\n",
    "\n",
    "After reading these articles, come up with five observations / reflections / questions and enter them below:\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "4. \n",
    "5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data Science Life Cycle <a id='section 1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulating a Question or Problem <a id='subsection 1a'></a>\n",
    "It is important to ask questions that will be informative and can be answered using the data. There are many different questions we could ask about music data. For example, there are many artists who want to find out how to get their music on Spotify's Discover Weekly playlist in order to gain exposure. Similarly, users love to see their *Spotify Wrapped* listening reports at the end of each year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Question:</b> Create a <i>new</i> research question that still remains unanswered after your initial investigation. For those who did not complete the Spotify project, work with another intern who has.\n",
    "   </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Original Question(s):** *here*\n",
    "\n",
    "\n",
    "**Data you would need:** *here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Question:</b> Now, taking a look at the data you already have, think of a few questions that you <b>could</b>\n",
    "    answer with the current data set.\n",
    "   </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your questions:** *here*\n",
    "\n",
    "**Columns / information you would need:** *here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquiring and Cleaning Data <a id='subsection 1b'></a>\n",
    "\n",
    "We'll be looking at song data from Spotify. You can find the raw data [here](https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-01-21). We've cleaned up the datasets a bit, and we will be investigating the popularity and the qualities of songs from this dataset.\n",
    "\n",
    "The following table, `spotify`, contains a list of tracks identified by their unique song ID along with attributes about that track.\n",
    "\n",
    "Here are the descriptions of the columns for your reference. (We will not be using all of these fields):\n",
    "\n",
    "|Variable Name   | Description |\n",
    "|--------------|------------|\n",
    "|`track_id` | \tSong unique ID |\n",
    "|`track_name` | Song Name |\n",
    "|`track_artist\t`| Song Artist |\n",
    "|`track_popularity` | Song Popularity (0-100) where higher is better |\n",
    "|`track_album_id`| Album unique ID |\n",
    "|`track_album_name` | Song album name |\n",
    "|`track_album_release_date`| Date when album released |\n",
    "|`playlist_name`| Name of playlist |\n",
    "|`playlist_id`| Playlist ID |\n",
    "|`playlist_genre`| Playlist genre |\n",
    "|`playlist_subgenre\t`|  Playlist subgenre |\n",
    "|`danceability`| Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable. |\n",
    "|`energy`| Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy. |\n",
    "|`key`| The estimated overall key of the track. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1. |\n",
    "|`loudness`|  The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db. |\n",
    "|`mode`|  Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0. |\n",
    "|`speechiness`|  Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks. |\n",
    "|`acousticness`|  A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic. |\n",
    "|`instrumentalness`| Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0. |\n",
    "|`liveness`| Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live. |\n",
    "|`valence`| A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry). |\n",
    "|`tempo`| The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration. |\n",
    "|`duration_ms`| Duration of song in milliseconds |\n",
    "|`creation_year`| Year when album was released |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spotify = Table.read_table('data/spotify.csv')\n",
    "spotify.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since today's research focuses on analyzing differences between audio features, we can remove a few of the unnecessary columns above. Run the following cell to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KEY\n",
    "\n",
    "desired_cols = make_array('track_id',\n",
    "                             'track_name',\n",
    "                             'playlist_genre',\n",
    "                             'playlist_subgenre',\n",
    "                             'danceability',\n",
    "                             'energy',\n",
    "                             'key',\n",
    "                             'loudness',\n",
    "                             'mode',\n",
    "                             'speechiness',\n",
    "                             'acousticness',\n",
    "                             'instrumentalness',\n",
    "                             'liveness',\n",
    "                             'valence',\n",
    "                             'tempo',\n",
    "                             'duration_ms',\n",
    "                         )\n",
    "spotify = spotify.select(desired_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conducting EDA: Rock vs. EDM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy for us as humans to distinguish between distinct music genres. We listen to the music and -- just like magic -- are able to tell what genre a song will fall into. However, how does a computer do this?\n",
    "\n",
    "In this section, we will investigate the audio features of different musical genres based on the songs in specific playlists. At the end, we will be able to see how a computer can tell the difference between **Rock** and **EDM**. We will be looking at the following feature columns:\n",
    "\n",
    "|Variable Name   | Description |\n",
    "|--------------|------------|\n",
    "|`playlist_genre`| Playlist genre |\n",
    "|`playlist_subgenre\t`|  Playlist subgenre |\n",
    "|`danceability`| Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable. |\n",
    "|`energy`| Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy. |\n",
    "|`key`| The estimated overall key of the track. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1. |\n",
    "|`loudness`|  The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db. |\n",
    "|`mode`|  Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0. |\n",
    "|`speechiness`|  Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks. |\n",
    "|`acousticness`|  A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic. |\n",
    "|`instrumentalness`| Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0. |\n",
    "|`liveness`| Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live. |\n",
    "|`valence`| A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry). |\n",
    "|`tempo`| The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration. |\n",
    "|`duration_ms`| Duration of song in milliseconds |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "        <b>Question:</b> Create two new tables called <code>rock</code> and <code>edm</code> which only have rows for the corresponding genre. Use the original table's <code>playlist_genre</code> column to obtain the genre.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KEY\n",
    "\n",
    "rock = spotify.where(\"playlist_genre\", are.equal_to(\"rock\"))\n",
    "rock.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KEY\n",
    "\n",
    "edm = spotify.where(\"playlist_genre\", are.equal_to(\"edm\"))\n",
    "edm.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Distributions \n",
    "\n",
    "Recall that you have used bar charts to visualize distributions of categorical data. The bar charts show how many individuals are in each category.\n",
    "\n",
    "Bar charts can also be used to visualize distributions of quantitative data. We can create \"categories\" by grouping the data into intervals known as *bins*. Then we can count how many individuals are in each bin, and visualize the counts in a bar chart.\n",
    "\n",
    "The function `quant_data_bar` has been written for this project, so that you can draw bar charts of quantitative data. It has two required arguments:\n",
    "\n",
    "- the name of the data table\n",
    "- the label of the column containing the data you want to visualize\n",
    "\n",
    "The function draws a bar chart of the data in the specified column, using 10 bins of equal width.\n",
    "\n",
    "Let's practice using this function before we dive into our analysis. Run the following cell to see a quantitative bar chart of the overall table's `danceability` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_data_bar(spotify, \"danceability\", num_bins=25, x_min=0, x_max=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "        <b>Question:</b> How can we interpret the graph we just created in the cell above? Why do the numbers range from 0 to 1. What does the shape of the plot mean?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*INSERT ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrumentalness\n",
    "\n",
    "A track's **instrumentalness** predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Question:</b> Create a quantitative bar chart for the <code>instrumentalness</code> column of the <code>rock</code> and <code>edm</code> tables. Play around with the <code>num_bins</code> argument until you have graphs that make sense.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#KEY\n",
    "\n",
    "quant_data_bar(rock, \"instrumentalness\", num_bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KEY\n",
    "\n",
    "quant_data_bar(edm, \"instrumentalness\", num_bins=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "        <b>Question:</b> Compare the quantitative bar charts between <b>Rock</b> and <b>EDM</b>. What do the data points around 0.8 represent for the <b>EDM</b> plot? Why might that exist?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*INSERT ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valence\n",
    "\n",
    "A track's **valence** is defined as a measure from 0.0 to 1.0 describing the musical positiveness conveyed by that track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Question:</b> Create a quantitative bar chart for the <code>valence</code> column of the <code>rock</code> and <code>edm</code> tables. Play around with the <code>num_bins</code> argument until you have graphs that make sense.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#KEY\n",
    "\n",
    "quant_data_bar(rock, \"valence\", num_bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KEY\n",
    "\n",
    "quant_data_bar(edm, \"valence\", num_bins=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "        <b>Question:</b> Compare the quantitative bar charts between <b>Rock</b> and <b>EDM</b>. How do the distributions differ? Why do more EDM songs have lower valence?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*INSERT ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tempo\n",
    "\n",
    "A track's **tempo** is measured in terms of beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Question:</b> Create a quantitative bar chart for the <code>tempo</code> column of the <code>rock</code> and <code>edm</code> tables. Play around with the <code>num_bins</code> argument until you have graphs that make sense.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#KEY\n",
    "\n",
    "quant_data_bar(rock, \"tempo\", num_bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KEY\n",
    "\n",
    "quant_data_bar(edm, \"tempo\", num_bins=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "        <b>Question:</b> Compare the quantitative bar charts between <b>Rock</b> and <b>EDM</b>. Specifically, look at the counts on the y-axis. Make some comments about the <i>spread</i> of the data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*INSERT ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song Duration\n",
    "\n",
    "A track's **song duration** is the length of the track, in milliseconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Question:</b> Create a quantitative bar chart for the <code>duration_ms</code> column of the <code>rock</code> and <code>edm</code> tables. Play around with the <code>num_bins</code> argument until you have graphs that make sense.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#KEY\n",
    "\n",
    "quant_data_bar(rock, \"duration_ms\", num_bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KEY\n",
    "\n",
    "quant_data_bar(edm, \"duration_ms\", num_bins=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "        <b>Question:</b> What conclusions can we make about the average song length for a rock song vs. an EDM song?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*INSERT ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Choice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have looked at some major differences between `instrumentalness`, `valence`, `tempo`, and `duration_ms` between rock songs and EDM songs, it is your turn to investigate! In the following cells, please work with your partner / group to decide on:\n",
    "- Two genres to investigate (not Rock and EDM)\n",
    "- Three audio features to look at for each genre\n",
    "\n",
    "After making your selections, carry out a similar process as above on your own. If you get stuck, or are unsure where to start, feel free to look at the above cells for some examples (rock, edm). Alternatively, please don't hesitate to reach out to your facilitators!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using prediction and inference to draw conclusions <a id='subsection 1a'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have looked at the different ways that audio features can be used to distinguish between certain musical genres, it is time to answer our intial question: \n",
    ">*How can machines tell the difference between different genres of music?*\n",
    "\n",
    "Answer the following questions to wrap up you extended research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "<b>Question:</b> After comparing audio features of rock and EDM songs, tell us something interesting about this data. What detail were you able to uncover?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*INSERT ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "<b>Question:</b> What are the limitations of using audio features to automatically identify genres? Can you think of an examples where this may be a bad approach?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*INSERT ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "<b>Question:</b> What other data could be used to determine a song's genre? How else can we use technology to group songs into categories?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*INSERT ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <h2 class=\"alert-heading\">Well done!</h2>\n",
    "    <p>In this report you used Spotify audio feature data from their Web API to determine how a machine can identify music genres automatically and how we can use data to <b>see</b> the difference between a rock song and an EDM song.\n",
    "    <hr>\n",
    "    <p> Notebook created for Berkeley Unboxing Data Science 2021 \n",
    "    <p> Adapted from Project 2: Spotify by Will Furtado with the support of Ani Adhikari, Deb Nolan, and Carlos Ortiz\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
